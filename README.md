# Explainable AI (xAI) 
*List of notebooks focused on Explainable AI (xAI)*

## Verfication vs. validation
- Verification is telling you whether you solved the equation **right**.
- Validation is telling you whether you solved the **right** equation. 
- Rather than focusing on explanations, ML practioner should really concentrate on is performance and whether that performance has been tested in a rigorous, scientific manner. There is a nice parallel of this way of thinking: in medicine is full of drugs and techniques that doctors use because they work, even though no one knows why acetaminophen has been used for a century to treat pain and inflammation, even though we still don’t fully understand the underlying mechanism.
- In other words, what we should care about when it comes to A.I. in the real world is not explanation. It is validation.


## A note on the notebook rendering
Each notebook has two versions (all python scripts are unaffected by this):
- One where all the markdown comments are rendered in black& white. These are placed in the folder named `GitHub_MD_rendering` where MD stands for MarkDown.
- One where all the markdown comments are rendered in coloured.


# References
- [What’s wrong with “explainable A.I.”](https://fortune.com/2022/03/22/ai-explainable-radiology-medicine-crisis-eye-on-ai/)